collins (1999) used a lexicalized approach, schiehlen (2004) used the manually annotated phrasal categories of the treebank. 
dependency fscore reached on the german negra treebank and collins et al (1999) 80.0% 
so, collins et al (1999) proposed a tag classification for parsing the czech treebank. 
the usage of special knowledge bases to determine projections of categories (xia and palmer, 2001) would have presupposed language-dependent knowledge, so we investigated two other options: flat rules (collins et al, 1999) and binary rules. 
another strategy that is often used in statistical parsing is markovization (collins, 1999): treebanks 1punctuation {$( $” $, $.} 
finally the placement of punctuation signs has a major impact on the performance of a parser (collins et al, 1999). 
we examine two state-of-the-art constituency-based parsers in this work: the collins czech parser (1999) and a version of the charniak parser (2001) that was modified to parse czech. 
although the results presented in (collins et al, 1999) used the reordering technique, we have experimented with his parser using the governor–raising technique and observe an increase in dependency accuracy. 
constituency parsing for dependency trees a pragmatic justification for using constituencybased parsers in order to predict dependency structures is that currently the best czech dependencytree parser is a constituency-based parser (collins et al, 1999; zeman, 2004). 
statistical parsing models have been shown to be successful in recovering labeled constituencies (collins, 2003; charniak and johnson, 2005; roark and collins, 2004) and have also been shown to be adequate in recovering dependency relationships (collins et al, 1999; levy and manning, 2004; dubey and keller, 2003). 
the trees are then transformed into penn treebank style constituencies using the technique described in (collins et al, 1999). 
47 feature type id description form f the fully inflected word form as it appears in the data lemma l the morphologically reduced lemma mtag t a subset of the morphological tag as described in (collins et al, 1999) pos p major part-of-speech tag (first field of the morphological tag) parsergov g true if candidate was proposed as governor by parser childcount c the number of children agreement a(x,y) check for case/number agreement between word x and y table 2: description of the classes of features used in all models, we include features containing the form, the lemma, the morphological tag, and the parsergov feature. 
collins (1999) explicitly added features to his parser to improve punctuation dependency parsing accuracy. 
in an attempt to extend a constituency-based parsing model to train on dependency trees, collins transforms the pdt dependency trees into constituency trees (collins et al, 1999). 
it is well known that dependency trees extracted from lexicalized phrase structure parsers (collins, 1999; charniak, 2000) typically are more accurate than those produced by pure dependency parsers (yamada and matsumoto, 2003). 
the czech parser of collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using english. 
in particular, we used the method of collins et al (1999) to simplify part-of-speech tags since the rich tags used by czech would have led to a large but rarely seen set of pos features. 
to create dependency structures from the penn treebank, we used the extraction rules of yamada and matsumoto (2003), which are an approximation to the lexicalization rules of collins (1999). 
the best phrase-structure parsing models represent generatively the joint probability p(x,y) of sentence x having the structure y (collins, 1999; charniak, 2000). 
we compared our system to the bikel re-implementation of the collins parser (bikel, 2004; collins, 1999) trained with the same head rules of our system. 
czech results for the czech data, we used the predefined training, development and testing split of the prague dependency treebank (hajiˇc et al, 2001), and the automatically generated pos tags supplied with the data, which we reduce to the pos tag set from collins et al (1999). 
although the best published results for the collins parser is 80% uas (collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of charniak's parser (charniak, 2000) performs at 84% (jan hajiˇc, pers. 
it is also true of the adaptation of the collins parser for czech (collins et al, 1999) and the finite-state dependency parser for turkish by oflazer (2003). 
table 4: parsing accuracy for mcle and mbl models, attachment score per sentence (per word in parentheses) if we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for swedish), we note that the best unlabeled attachment score is lower than for english, where the best results are above 90% (attachment score per word) (collins et al, 1999; yamada and matsumoto, 2003), but higher than for czech (collins et al, 1999). 
thus, the penn treebank of american english (marcus et al, 1993) has been used to train and evaluate the best available parsers of unrestricted english text (collins, 1999; charniak, 2000). 
more precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing (eisner, 1996; collins et al, 1999). 
however, since most previous studies instead use the mean attachment score per word (eisner, 1996; collins et al, 1999), we will give this measure as well. 
unlike most previous work on data-driven dependency parsing (eisner, 1996; collins et al, 1999; yamada and matsumoto, 2003; nivre, 2003), we assume that dependency graphs are labeled with dependency types, although the evaluation will give results for both labeled and unlabeled representations. 
the part-of-speech tagging used (both in training and testing) is the hmm tagging distributed with the treebank, with a tagging accuracy of 94.1%, and with the tagset compressed to 61 tags as in collins et al (1999). 
this is well illustrated by the collins parser (collins, 1997; collins, 1999), scrutinized by bikel (2004), where several transformations are applied in order to improve the analysis of noun phrases, coordination and punctuation. 
more specifically for pdt, collins et al (1999) relabel coordinated phrases after converting dependency structures to phrase structures, and zeman (2004) uses a kind of pattern matching, based on frequencies of the parts-of-speech of conjuncts and conjunctions. 
dependency-based statistical language modeling and parsing have also become quite popular in statistical natural language processing (lafferty, sleator, and temperley 1992; eisner 1996; chelba et al 1997; collins 1996; collins et al 1999). 
we find lexical heads in penn treebank data using the rules described in appendix a of collins (1999). 
collins et al (1999) describe how the models in the current article were applied to parsing czech. 
the appendices of collins (1999) give a precise description of the parsing algorithms, an analysis of their computational complexity, and also a description of the pruning methods that are employed. 
see appendix a of collins (1999) for a description of how the head rules treat phrases involving coordination. 
we give a 1 much of this article is an edited version of chapters 7 and 8 of collins (1999). 
for discussion of additional related work, chapter 4 of collins (1999) attempts to give a comprehensive review of work on statistical parsing up to around 1998. 
as a preprocessing step, the 14 in collins (1999) we erroneously stated that all words occuring less than five times in training data were classified as “unknown.” 
see collins (1999) for a full description of the parsing algorithms. 
dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as czech (collins et al, 1999), bulgarian (marinov and nivre, 2005), and turkish (eryi˘git and oflazer, 2006). 
although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the 6this f-measure is based on the recall and precision figures reported in figure 7.15 in collins (1999). 
unlabeled attachment score (uas): the proportion of words that are assigned the correct head (or no head if the word is a root) (eisner, 1996; collins et al, 1999). 
for the larger b set, our best parser achieves an f-measure of 96.9% (dep labels included), which can be compared with 97.0% for a similar (but larger) set of labels in collins (1999).6 
used for training and section 23 for testing (collins, 1999; charniak, 2000). 
we are grateful to yamada and matsumoto for letting us use their rule set, which is a slight modification of the rules used by collins (1999). 
we should note that the results in collins et al (1999) are different then reported here due to different training and testing data sets. 
to reduce sparseness, our features rely only on the reduced pos tag set from collins et al (1999). 
coll1999: the projective lexicalized phrase-structure parser of collins et al (1999). 
a non-projective example from the czech prague dependency treebank (hajiˇc et al, 2001) is also shown in figure 2. most previous dependency parsing models have focused on projective trees, including the work of eisner (1996), collins et al (1999), yamada and matsumoto (2003), nivre and scholz (2004), and mcdonald et al (2005). 
furthermore, we can also see that the mst parsers perform favorably compared to the more powerful lexicalized phrase-structure parsers, such as those presented by collins et al (1999) and zeman (2004) that use expensive o(n5) parsing algorithms. 
collins (1997)'s parser and its reimplementation and extension by bikel (2002) have by now been applied to a variety of languages: english (collins, 1999), czech (collins et al, 1999), german (dubey and keller, 2003), spanish (cowan and collins, 2005), french (arun and keller, 2005), chinese (bikel, 2002) and, according to dan bikel's web page, arabic. 
collins et al (1999) applied the parser of collins (1997) developed for english, to czech, and found thatthe performance wassubstantially lower when compared to the results for english. 
described in (haji et al, 1998; collins et al, 1999). 
