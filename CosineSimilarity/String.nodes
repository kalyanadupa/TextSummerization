0 collins (1999) used a lexicalized approach, schiehlen (2004) used the manually annotated phrasal categories of the treebank. 
1 dependency fscore reached on the german negra treebank and collins et al (1999) 80.0% 
2 so, collins et al (1999) proposed a tag classification for parsing the czech treebank. 
3 the usage of special knowledge bases to determine projections of categories (xia and palmer, 2001) would have presupposed language-dependent knowledge, so we investigated two other options: flat rules (collins et al, 1999) and binary rules. 
4 another strategy that is often used in statistical parsing is markovization (collins, 1999): treebanks 1punctuation {$( $” $, $.} 
5 finally the placement of punctuation signs has a major impact on the performance of a parser (collins et al, 1999). 
6 we examine two state-of-the-art constituency-based parsers in this work: the collins czech parser (1999) and a version of the charniak parser (2001) that was modified to parse czech. 
7 although the results presented in (collins et al, 1999) used the reordering technique, we have experimented with his parser using the governor–raising technique and observe an increase in dependency accuracy. 
8 constituency parsing for dependency trees a pragmatic justification for using constituencybased parsers in order to predict dependency structures is that currently the best czech dependencytree parser is a constituency-based parser (collins et al, 1999; zeman, 2004). 
9 statistical parsing models have been shown to be successful in recovering labeled constituencies (collins, 2003; charniak and johnson, 2005; roark and collins, 2004) and have also been shown to be adequate in recovering dependency relationships (collins et al, 1999; levy and manning, 2004; dubey and keller, 2003). 
10 the trees are then transformed into penn treebank style constituencies using the technique described in (collins et al, 1999). 
11 47 feature type id description form f the fully inflected word form as it appears in the data lemma l the morphologically reduced lemma mtag t a subset of the morphological tag as described in (collins et al, 1999) pos p major part-of-speech tag (first field of the morphological tag) parsergov g true if candidate was proposed as governor by parser childcount c the number of children agreement a(x,y) check for case/number agreement between word x and y table 2: description of the classes of features used in all models, we include features containing the form, the lemma, the morphological tag, and the parsergov feature. 
12 collins (1999) explicitly added features to his parser to improve punctuation dependency parsing accuracy. 
13 in an attempt to extend a constituency-based parsing model to train on dependency trees, collins transforms the pdt dependency trees into constituency trees (collins et al, 1999). 
14 it is well known that dependency trees extracted from lexicalized phrase structure parsers (collins, 1999; charniak, 2000) typically are more accurate than those produced by pure dependency parsers (yamada and matsumoto, 2003). 
15 the czech parser of collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using english. 
16 in particular, we used the method of collins et al (1999) to simplify part-of-speech tags since the rich tags used by czech would have led to a large but rarely seen set of pos features. 
17 to create dependency structures from the penn treebank, we used the extraction rules of yamada and matsumoto (2003), which are an approximation to the lexicalization rules of collins (1999). 
18 the best phrase-structure parsing models represent generatively the joint probability p(x,y) of sentence x having the structure y (collins, 1999; charniak, 2000). 
19 we compared our system to the bikel re-implementation of the collins parser (bikel, 2004; collins, 1999) trained with the same head rules of our system. 
20 czech results for the czech data, we used the predefined training, development and testing split of the prague dependency treebank (hajiˇc et al, 2001), and the automatically generated pos tags supplied with the data, which we reduce to the pos tag set from collins et al (1999). 
21 although the best published results for the collins parser is 80% uas (collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of charniak's parser (charniak, 2000) performs at 84% (jan hajiˇc, pers. 
22 it is also true of the adaptation of the collins parser for czech (collins et al, 1999) and the finite-state dependency parser for turkish by oflazer (2003). 
23 table 4: parsing accuracy for mcle and mbl models, attachment score per sentence (per word in parentheses) if we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for swedish), we note that the best unlabeled attachment score is lower than for english, where the best results are above 90% (attachment score per word) (collins et al, 1999; yamada and matsumoto, 2003), but higher than for czech (collins et al, 1999). 
24 thus, the penn treebank of american english (marcus et al, 1993) has been used to train and evaluate the best available parsers of unrestricted english text (collins, 1999; charniak, 2000). 
25 more precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing (eisner, 1996; collins et al, 1999). 
26 however, since most previous studies instead use the mean attachment score per word (eisner, 1996; collins et al, 1999), we will give this measure as well. 
27 unlike most previous work on data-driven dependency parsing (eisner, 1996; collins et al, 1999; yamada and matsumoto, 2003; nivre, 2003), we assume that dependency graphs are labeled with dependency types, although the evaluation will give results for both labeled and unlabeled representations. 
28 the part-of-speech tagging used (both in training and testing) is the hmm tagging distributed with the treebank, with a tagging accuracy of 94.1%, and with the tagset compressed to 61 tags as in collins et al (1999). 
29 this is well illustrated by the collins parser (collins, 1997; collins, 1999), scrutinized by bikel (2004), where several transformations are applied in order to improve the analysis of noun phrases, coordination and punctuation. 
30 more specifically for pdt, collins et al (1999) relabel coordinated phrases after converting dependency structures to phrase structures, and zeman (2004) uses a kind of pattern matching, based on frequencies of the parts-of-speech of conjuncts and conjunctions. 
31 dependency-based statistical language modeling and parsing have also become quite popular in statistical natural language processing (lafferty, sleator, and temperley 1992; eisner 1996; chelba et al 1997; collins 1996; collins et al 1999). 
32 we find lexical heads in penn treebank data using the rules described in appendix a of collins (1999). 
33 collins et al (1999) describe how the models in the current article were applied to parsing czech. 
34 the appendices of collins (1999) give a precise description of the parsing algorithms, an analysis of their computational complexity, and also a description of the pruning methods that are employed. 
35 see appendix a of collins (1999) for a description of how the head rules treat phrases involving coordination. 
36 we give a 1 much of this article is an edited version of chapters 7 and 8 of collins (1999). 
37 for discussion of additional related work, chapter 4 of collins (1999) attempts to give a comprehensive review of work on statistical parsing up to around 1998. 
38 as a preprocessing step, the 14 in collins (1999) we erroneously stated that all words occuring less than five times in training data were classified as “unknown.” 
39 see collins (1999) for a full description of the parsing algorithms. 
40 dependency-based representations have become increasingly popular in syntactic parsing, especially for languages that exhibit free or flexible word order, such as czech